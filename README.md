## Natural Language Queries for Egocentric Vision, using VSLNet

This project explores how to understand egocentric videos using natural language queries. The main goal was to train models to accurately localize a video segment that answers a given question. To do this, the Ego4D dataset and its Natural Language Query (NLQ) benchmark task was used. A comparative study of two prominent models, VSLBase and VSLNet, was conducted using visual features from Omnivore and EgoVLP respectively. For the input queries, two different methods for encoding text were compared and BERT was identified as the most effective one. As an extension to the project, new training data was generated using a large language model, Gemma-2b, based on video narrations that are not part of the original NLQ benchmark. It was found that using these automatically generated queries for pretraining VSLNet significantly boosts the model's performance upon fine-tuning on the official Ego4D NLQ data. Overall, the results show that combining VSLNet with EgoVLP features and BERT, along with pretraining on automatically generated queries, leads to more accurate understanding of egocentric videos through natural language.

### How to run the project - step by step
All the following steps should be done in the google colab notebook "MLDL_project_Egocentric_NLQ.ipynb". Link to colab notebook: https://colab.research.google.com/drive/1jm8BmMTGS9im7ODWMRKl8Rx96RNqIamf?usp=sharing
#### 1. Training VSLNet with Omnivore features and Bert as text encoder
Run the colab notebook with all the default files in the VSLNet folder, which means nothing has to be done with the files after cloning the repository into the     notebook. Do not run the two cells that are for pretraining. Look at the comments in the cells of the notebook and make sure to comment out/comment in according to the instructions for using Omnivore features. 
#### 2. Training VSLNet with EgoVLP features and Bert as text encoder
Make sure to have the EgoVLP features in a .tar.gz file on your google drive. Run the colab notebook with all the default files in the VSLNet folder, which means nothing has to be done with the files after cloning the repository into the notebook. Do not run the two cells that are for pretraining. Look at the comments in the cells of the notebook and make sure to follow the instructions for using EgoVLP features.
#### 3. Training VSLNet with EgoVLP features and GloVe as text encoder
Follow the same steps as in 2., but make to have the file glove.840B.300d.zip on your google drive. Extract that file into the notebook. Look at the comments that have to do with using GloVe and follow those instructions. Make sure to change the --predictor to any string other than "bert". 
#### 4. Training VSLBase with Omnivore/EgoVLP and Bert
Depending on which features you are using, follow the steps for either Omnivore or EgoVLP features. After cloning this repository, replace the files VSLNet/model/VSLNet.py, VSLNet/utils/runner_utils.py and VSLNet/main.py with the corresponding altered files in the folder VSLBase. Do not run the two cells that are for pretraining.
#### 5. Running the extension
To create the new .json file with automatically generated queries you can run the code in the end of the notebook under the headline "Extension - Automatic Queries Generation using LLMs". Otherwise you can just upload the file augmented_nlq.json from this repository to the colab notebook. In this extension VSLNet was used with the EgoVLP features and Bert as text encoder since that setup had proven to have the best performance. To pretrain the network on the new queries in the file augmented_json.py the file prepare_ego4d_dataset_pretrain.py has to be used instead of prepare_ego4d_dataset.py, and the cell that runs this script has to be run. Then data_gen.py has be deleted from the notebook and data_gen_pretrain.py has to be used instead but its filename has to changed to data_gen.py before running the code. Then the training cell using main_pretrain.py should be run to pretrain the network. The pretrained model will be saved in a file with the name vslnet_3230.t7 in this path: /content/nlq_official_v1/checkpoints//egovlp_fp16/vslnet_nlq_official_v1_egovlp_fp16_official_128_bert/model/vslnet_3230.t7. This file and the configs.json file in the same folder was downloaded. To then be able to finetune this pretrained model on the original NLQ data the entire notebook had to be rerun from the start and the files vslnet_3230.t7 and configs.json were uploaded to the path /content/nlq_official_v1/checkpoints//egovlp_fp16/vslnet_nlq_official_v1_egovlp_fp16_official_128_bert/model/. Then the original prepare_ego4d_dataset.py is run and then the training cell is run with the file main_finetune.py (either change the name of the file to main.py or change to main_finetune.py in the notebook cell). 


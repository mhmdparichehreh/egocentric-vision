{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmoup7_Xsbc4"
      },
      "source": [
        "# Ego4D Moments Benchmark (NLQ) Quickstart\n",
        "\n",
        "Please set your resources to GPU (Runtime -> Change runtime type -> GPU).\n",
        "\n",
        "This quickstart will show:\n",
        "1. An overview of the training data\n",
        "2. How to train the baseline (VSLNet)\n",
        "\n",
        "To begin: add your **access keys** below, change your Runtime Type to **GPU**, and run cells **one by one** as you read through. This helps avoid timeouts since Colab gives more GPU cycles to interactive notebooks.\n",
        "\n",
        "## Resources\n",
        "- [Baseline Repo](https://github.com/EGO4D/episodic-memory/tree/main/NLQ/VSLNet)\n",
        "- [Docs](https://ego4d-data.org/docs/benchmarks/episodic-memory/)\n",
        "- [EvalAI Challenge](https://eval.ai/web/challenges/challenge-page/1629/overview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBsjg8pN0knX"
      },
      "source": [
        "## Download Data and Setup Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcNVEU5Z8f5p"
      },
      "source": [
        "### **Fill In Your Access Info Here**\n",
        "If you don't have access and secret keys, first sign the Ego4D License at [ego4ddataset.com](https://ego4ddataset.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTSvhBsBvnXy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = \"AKIATEEVKTGZE3L4UJFX\"\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcIg7gNx82Bq"
      },
      "source": [
        "### **Set up CLIs and Download Annotations + Repo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-D9Jm-l162m",
        "outputId": "b62ea8fc-7284-4841-ac18-865c61bf7dc4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 67.2M  100 67.2M    0     0   239M      0 --:--:-- --:--:-- --:--:--  240M\n"
          ]
        }
      ],
      "source": [
        "# Download the AWS and Ego4D CLIs, then download the annotations locally\n",
        "%%bash\n",
        "\n",
        "# Set up the AWS CLI\n",
        "curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
        "unzip -o awscliv2.zip >/dev/null\n",
        "sudo ./aws/install >/dev/null 2>&1\n",
        "aws configure set aws_access_key_id \"$AWS_ACCESS_KEY_ID\" && aws configure set aws_secret_access_key \"$AWS_SECRET_ACCESS_KEY\"\n",
        "rm \"awscliv2.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tXEDSW50Ebd"
      },
      "source": [
        "### Install the ego4d CLI and Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg6Xt1p-On-a",
        "outputId": "934f8f2f-1a1d-4109-9682-12fad871dae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ego4d\n",
            "  Downloading ego4d-1.7.3.tar.gz (94 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/94.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.5/94.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3 (from ego4d)\n",
            "  Downloading boto3-1.38.13-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from ego4d) (4.67.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from ego4d) (2024.11.6)\n",
            "Collecting dataclasses_json (from ego4d)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting iopath (from ego4d)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botocore<1.39.0,>=1.38.13 (from boto3->ego4d)\n",
            "  Downloading botocore-1.38.13-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->ego4d)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.13.0,>=0.12.0 (from boto3->ego4d)\n",
            "  Downloading s3transfer-0.12.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses_json->ego4d)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses_json->ego4d)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath->ego4d) (4.13.2)\n",
            "Collecting portalocker (from iopath->ego4d)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.13->boto3->ego4d) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.13->boto3->ego4d) (2.4.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses_json->ego4d) (24.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses_json->ego4d)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.13->boto3->ego4d) (1.17.0)\n",
            "Downloading boto3-1.38.13-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading botocore-1.38.13-py3-none-any.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m122.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.12.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: ego4d, iopath\n",
            "  Building wheel for ego4d (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ego4d: filename=ego4d-1.7.3-py3-none-any.whl size=118281 sha256=42c39b6c5c579018bb280024bd4dbef33909ed6adb1197643331f6b83142e7b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/f5/df/07/e6fbe27d3ca2410baf3e04485d356eda052539f27ac2d52d8a\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=c0900117cb7329e3b5abef96e0503140a6f51e31d716c349ae39ef4c1ab801c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built ego4d iopath\n",
            "Installing collected packages: portalocker, mypy-extensions, marshmallow, jmespath, typing-inspect, iopath, botocore, s3transfer, dataclasses_json, boto3, ego4d\n",
            "Successfully installed boto3-1.38.13 botocore-1.38.13 dataclasses_json-0.6.7 ego4d-1.7.3 iopath-0.1.10 jmespath-1.0.1 marshmallow-3.26.1 mypy-extensions-1.1.0 portalocker-3.1.1 s3transfer-0.12.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# Set up the Ego4D CLI\n",
        "!pip install ego4d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcKr9i88KMaa",
        "outputId": "7e38f763-a851-43d8-ba30-fa2d14be9d6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Non-standard Dataset Specfied (Allowed, will attempt download): ['omnivore_video_swinl_fp16']\n",
            "Datasets to download: {'annotations', 'omnivore_video_swinl_fp16'}\n",
            "Download Path: /content/ego4d_data/v1\n",
            "Downloading Ego4D metadata json..\n",
            "Ego4D Metadata: /content/ego4d_data/ego4d.json\n",
            "Checking requested datasets and versions...\n",
            "Created download directory for version 'v1' of dataset: 'annotations' at: /content/ego4d_data/v1/annotations\n",
            "Benchmarks specified but ignored without a benchmarks field in manifest.\n",
            "Created download directory for version 'v1' of dataset: 'omnivore_video_swinl_fp16' at: /content/ego4d_data/v1/omnivore_video_swinl_fp16\n",
            "Filtering by benchmarks: ['nlq']\n",
            "Retrieving object metadata from S3...\n",
            "100% 1290/1290 [00:02<00:00, 437.96object/s]\n",
            "Checking if latest file versions are already downloaded...\n",
            "  0% 0/1290 [00:00<?, ?file/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: ego4d-consortium-sharing.s3.us-west-1.amazonaws.com. Connection pool size: 10\n",
            "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: ego4d-consortium-sharing.s3.us-west-1.amazonaws.com. Connection pool size: 10\n",
            "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: ego4d-consortium-sharing.s3.us-west-1.amazonaws.com. Connection pool size: 10\n",
            "  1% 10/1290 [00:00<01:45, 12.17file/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: ego4d-consortium-sharing.s3.us-west-1.amazonaws.com. Connection pool size: 10\n",
            "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: ego4d-consortium-sharing.s3.us-west-1.amazonaws.com. Connection pool size: 10\n",
            "  5% 67/1290 [00:05<01:14, 16.32file/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: ego4d-bristol.s3.eu-west-2.amazonaws.com. Connection pool size: 10\n",
            "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: ego4d-bristol.s3.eu-west-2.amazonaws.com. Connection pool size: 10\n",
            "100% 1290/1290 [00:10<00:00, 119.62file/s]\n",
            "No existing videos to filter.\n",
            "Downloading 1290 files..\n",
            "100% 12.8G/12.8G [02:40<00:00, 34.0MiB/s]Checking file integrity...\n",
            "100% 12.8G/12.8G [02:40<00:00, 85.9MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the Ego4D Annotations to ego4d_data/\n",
        "!ego4d --output_directory=\"/content/ego4d_data/\" --version v1 --datasets annotations omnivore_video_swinl_fp16 --benchmarks nlq -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBPJoyYD2D7w"
      },
      "source": [
        "### Check Downloaded Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS0ADqVR1tkB",
        "outputId": "353c8e98-0740-4498-86bb-a37060157b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nlq_test_unannotated.json\n",
            "nlq_train.json\n",
            "nlq_val.json\n"
          ]
        }
      ],
      "source": [
        "# Ensure we have downloaded the files correctly\n",
        "!ls /content/ego4d_data/v1/annotations | grep nlq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt5hhPdK2C3X",
        "outputId": "694150cf-4eff-41ce-caaa-beddb6afbba7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1261\n"
          ]
        }
      ],
      "source": [
        "!ls /content/ego4d_data/v1/omnivore_video_swinl_fp16 | wc -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE4fDxQk1KAm"
      },
      "source": [
        "### Clone the Episodic Memory Baseline Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zApdimh4TQdm"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "git clone https://github.com/EGO4D/episodic-memory\n",
        "cd episodic-memory\n",
        "git pull\n",
        "git checkout nlq_fixes_and_fp16_support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lenDRgBjWVg"
      },
      "source": [
        "# Stats for Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lg3GlF2q5DS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r64o6AajXe3"
      },
      "outputs": [],
      "source": [
        "ann_data = json.load(open(\"/content/ego4d_data/v1/annotations/nlq_train.json\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gaf-Ly0pDpf"
      },
      "outputs": [],
      "source": [
        "# please see https://ego4d-data.org/docs/data/annotations-schemas/\n",
        "\n",
        "anns = []\n",
        "for vid in ann_data[\"videos\"]:\n",
        "    for clip in vid[\"clips\"]:\n",
        "        for ann in clip[\"annotations\"]:\n",
        "            for query in ann[\"language_queries\"]:\n",
        "                anns.append({\n",
        "                    \"query_start_time_sec\": clip[\"video_start_sec\"],\n",
        "                    \"query_end_time_sec\": clip[\"video_end_sec\"],\n",
        "                    \"query_response_start_time_sec\": query[\"video_start_sec\"],\n",
        "                    \"query_response_end_time_sec\": query[\"video_end_sec\"],\n",
        "                    \"query_template\": query.get(\"template\", None),\n",
        "                    \"query\": query.get(\"query\", None),\n",
        "                })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjO16XNNqaRb"
      },
      "outputs": [],
      "source": [
        "num_queries = len(anns)\n",
        "relative_query_sizes = np.array([\n",
        "    (a[\"query_response_end_time_sec\"] - a[\"query_response_start_time_sec\"]) / (a[\"query_end_time_sec\"] - a[\"query_start_time_sec\"])\n",
        "    for a in anns\n",
        "])\n",
        "query_sizes = np.array([\n",
        "    (a[\"query_response_end_time_sec\"] - a[\"query_response_start_time_sec\"])\n",
        "    for a in anns\n",
        "])\n",
        "clip_sizes = np.array([\n",
        "    (a[\"query_end_time_sec\"] - a[\"query_start_time_sec\"])\n",
        "    for a in anns\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCckfo1SsOvJ"
      },
      "source": [
        "## Query / Response Durations\n",
        "\n",
        "Here we can see that there are some queries with 0s. When training it is reccomended to remove them from the set. For VSLNet you can provide `--remove_empty_queries_from train` to `main.py` (e.g. if you are modifying this codebase)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV5CcoGurjjX"
      },
      "outputs": [],
      "source": [
        "query_sizes.max(), query_sizes.min(), query_sizes.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpnctDcBsnEy"
      },
      "outputs": [],
      "source": [
        "# less than or equal to 4 frames => 9% of training data\n",
        "(query_sizes <= 4/30).sum() / len(relative_query_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbHI3vJItI6Y"
      },
      "source": [
        "## Distribution of Queries (relative)\n",
        "\n",
        "Here is a histogram plot of the relative query size to the clip size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVMJOWYUt89N"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (16, 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLCvNnsHrMoc"
      },
      "outputs": [],
      "source": [
        "plt.hist(relative_query_sizes[relative_query_sizes < 0.2], density=True, bins=128)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_fOTeb5r9-a"
      },
      "outputs": [],
      "source": [
        "plt.hist(relative_query_sizes[relative_query_sizes > 0.2], density=True, bins=128)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-YWfHSiteoC"
      },
      "source": [
        "## Clip Sizes\n",
        "\n",
        "Clips are 522s on average, with most clips being 480s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H-JCUd1th5Y"
      },
      "outputs": [],
      "source": [
        "clip_sizes.mean(), clip_sizes.max(), clip_sizes.min(), clip_sizes.std(), np.median(clip_sizes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOe9_BVCtZkV"
      },
      "outputs": [],
      "source": [
        "plt.hist(clip_sizes)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPrnrhRRJhdC"
      },
      "source": [
        "# Prepare Dataset\n",
        "\n",
        "The NLQ baseline repository for VSLNet requires you to prepare the data for training and evaluation purposes. From the [README.md](https://github.com/EGO4D/episodic-memory/blob/main/NLQ/VSLNet/README.md#preparation) we need to run the `prepare_ego4d_dataset.py` script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AXdrwdR2W5v"
      },
      "source": [
        "### Setup Environment Variables for NLQ\n",
        "\n",
        "First let's setup some environment variables and setup the paths as NLQ's scripts will expect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyZZBxFd-DsW"
      },
      "outputs": [],
      "source": [
        "with open(\"vars.sh\", \"w\") as out_f:\n",
        "  out_f.write(\"\"\"\n",
        "export NAME=omnivore_video_fp16\n",
        "export TASK_NAME=nlq_official_v1_$NAME\n",
        "export BASE_DIR=data/dataset/nlq_official_v1_$NAME\n",
        "export FEATURE_BASE_DIR=data/features/nlq_official_v1_$NAME/\n",
        "export FEATURE_DIR=$FEATURE_BASE_DIR/video_features\n",
        "export MODEL_BASE_DIR=/content/nlq_official_v1/checkpoints/\n",
        "\n",
        "cd episodic-memory/NLQ/VSLNet\n",
        "\"\"\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4rHsY8I5PG8"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "source vars.sh\n",
        "\n",
        "echo $FEATURE_BASE_DIR\n",
        "mkdir -p $FEATURE_BASE_DIR\n",
        "ln -s /content/ego4d_data/v1/omnivore_video_swinl_fp16 $FEATURE_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b6PS8kYR5wHY",
        "outputId": "d0228886-af20-4de7-f0e0-6a1fc6c7855d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting submitit\n",
            "  Downloading submitit-1.5.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Collecting terminaltables\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from submitit) (3.1.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.11/dist-packages (from submitit) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Downloading submitit-1.5.2-py3-none-any.whl (74 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74.9/74.9 kB 3.4 MB/s eta 0:00:00\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 4.2 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 123.9 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 100.5 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 45.0 MB/s eta 0:00:00\n",
            "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 2.0 MB/s eta 0:00:00\n",
            "Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 5.5 MB/s eta 0:00:00\n",
            "Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 14.5 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 7.5 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 6.3 MB/s eta 0:00:00\n",
            "Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 106.3 MB/s eta 0:00:00\n",
            "Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: terminaltables, submitit, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 submitit-1.5.2 terminaltables-3.1.10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "bash: line 1: fg: no job control\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "%%capture\n",
        "\n",
        "source vars.sh\n",
        "pip install nltk submitit torch torchaudio torchvision tqdm transformers tensorboard Pillow terminaltables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMKYQdnHjQCh"
      },
      "source": [
        "# Train and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1tuBAxl2hG4"
      },
      "source": [
        "## Run the Prepare Script\n",
        "\n",
        "This script will take a while to run and may not output progress until it is done. Please be patient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xE0gCoFb2kYq"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "source vars.sh\n",
        "\n",
        "python utils/prepare_ego4d_dataset.py \\\n",
        "    --input_train_split /content/ego4d_data/v1/annotations/nlq_train.json \\\n",
        "    --input_val_split /content/ego4d_data/v1/annotations/nlq_val.json \\\n",
        "    --input_test_split /content/ego4d_data/v1/annotations/nlq_test_unannotated.json \\\n",
        "    --video_feature_read_path $FEATURE_DIR \\\n",
        "    --clip_feature_save_path $FEATURE_BASE_DIR/official \\\n",
        "    --output_save_path $BASE_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdMsEL7q2uq2"
      },
      "source": [
        "## Train a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vvSjO3-7OeG"
      },
      "source": [
        "Please note:\n",
        "1. These are *not* the parameters for the original baseline model in the Ego4D whitepaper.\n",
        "2. Omnivore video features are used (slowfast was originally used), and their FP16 variant. This is due to free colab constraints (100GB disk space).\n",
        "\n",
        "Omnivore video features do out-perform slowfast features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtAHrCP5ij1Q"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJzhRfjk_qEQ"
      },
      "source": [
        "You may have to re-run this cell after you run the training script. You can try to reload data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZE3fXIV4gat"
      },
      "outputs": [],
      "source": [
        "!kill 11504"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT2ABqXvimuH"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/episodic-memory/NLQ/VSLNet/runs/\n",
        "%tensorboard --logdir /content/episodic-memory/NLQ/VSLNet/runs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVHPgeDz8r7U"
      },
      "source": [
        "Unfortunately due to colab and the time taken in the below script (it first saves additional metadata to disk) - the cell below takes a while to get started. Please be patient when running it may take at least 30 minutes. You may get timed out from colab.\n",
        "\n",
        "You can tell if the training is started by inspecting the filesystem on the left hand side. The directory: `episodic-memory/NLQ/VSLNet/runs` will populate with a subdirectory for the tensorboard logdir.\n",
        "\n",
        "Please note, these are *not* the hyper parameters used for the baseline. The following uses an aggressive learning rate and a low number of epochs, to converge faster so you don't have to wait as long for this cell to finish. :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LXflBep21gl"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "source vars.sh\n",
        "\n",
        "# machine parameters\n",
        "export DATALOADER_WORKERS=1\n",
        "export NUM_WORKERS=2\n",
        "export VAL_JSON_PATH=\"/content/ego4d_data/v1/annotations/nlq_val.json\"\n",
        "\n",
        "# hyper parameters\n",
        "export BATCH_SIZE=32\n",
        "export DIM=128\n",
        "export NUM_EPOCH=10\n",
        "export MAX_POS_LEN=128\n",
        "export INIT_LR=0.0025\n",
        "\n",
        "export TB_LOG_NAME=\"${NAME}_bs${BATCH_SIZE}_dim${DIM}_epoch${NUM_EPOCH}_ilr${INIT_LR}\"\n",
        "\n",
        "python main.py \\\n",
        "    --task $TASK_NAME \\\n",
        "    --predictor bert \\\n",
        "    --dim $DIM \\\n",
        "    --mode train \\\n",
        "    --video_feature_dim 1536 \\\n",
        "    --max_pos_len $MAX_POS_LEN \\\n",
        "    --init_lr $INIT_LR \\\n",
        "    --epochs $NUM_EPOCH \\\n",
        "    --batch_size $BATCH_SIZE \\\n",
        "    --fv official \\\n",
        "    --num_workers $NUM_WORKERS \\\n",
        "    --data_loader_workers $DATALOADER_WORKERS \\\n",
        "    --model_dir $MODEL_BASE_DIR/$NAME \\\n",
        "    --eval_gt_json $VAL_JSON_PATH \\\n",
        "    --log_to_tensorboard $TB_LOG_NAME \\\n",
        "    --tb_log_freq 5 \\\n",
        "    --remove_empty_queries_from train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMulhc05eF6Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
